total.time
rpart.cv.1
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 1)
stopCluster(cl)
start.time <- Sys.time()
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 2)
stopCluster(cl)
total.time <- Sys.time() - start.time
total.time
rpart.cv.1
help(memory.size)
memory.size()
memory.size(max=FALSE)
memory.limit()
# Load up the .CSV data and explore in RStudio.
spam.raw <- read.csv("C:/Users/siarkmi2/Documents/R/spam.csv", stringsAsFactors = FALSE)
View(spam.raw)
# Clean up the data frame and view our handiwork.
spam.raw <- spam.raw[, 1:2]
names(spam.raw) <- c("Label", "Text")
View(spam.raw)
# Check data to see if there are missing values.
length(which(!complete.cases(spam.raw)))
# Convert our class label into a factor.
spam.raw$Label <- as.factor(spam.raw$Label)
# The first step, as always, is to explore the data.
# First, let's take a look at distibution of the class labels (i.e., ham vs. spam).
prop.table(table(spam.raw$Label))
# Next up, let's get a feel for the distribution of text lengths of the SMS
# messages by adding a new feature for the length of each message.
spam.raw$TextLength <- nchar(spam.raw$Text)
summary(spam.raw$TextLength)
# Visualize distribution with ggplot2, adding segmentation for ham/spam.
library(ggplot2)
ggplot(spam.raw, aes(x = TextLength, fill = Label)) +
theme_bw() +
geom_histogram(binwidth = 5) +
labs(y = "Text Count", x = "Length of Text",
title = "Distribution of Text Lengths with Class Labels")
# At a minimum we need to split our data into a training set and a
# test set. In a true project we would want to use a three-way split
# of training, validation, and test.
#
# As we know that our data has non-trivial class imbalance, we'll
# use the mighty caret package to create a randomg train/test split
# that ensures the correct ham/spam class label proportions (i.e.,
# we'll use caret for a random stratified split).
library(caret)
help(package = "caret")
# Use caret to create a 70%/30% stratified split. Set the random
# seed for reproducibility.
set.seed(32984)
indexes <- createDataPartition(spam.raw$Label, times = 1,
p = 0.7, list = FALSE)
train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]
# Verify proportions.
prop.table(table(train$Label))
prop.table(table(test$Label))
# Text analytics requires a lot of data exploration, data pre-processing
# and data wrangling. Let's explore some examples.
# HTML-escaped ampersand character.
train$Text[21]
# HTML-escaped '<' and '>' characters. Also note that Mallika Sherawat
# is an actual person, but we will ignore the implications of this for
# this introductory tutorial.
train$Text[38]
# A URL.
train$Text[357]
# There are many packages in the R ecosystem for performing text
# analytics. One of the newer packages in quanteda. The quanteda
# package has many useful functions for quickly and easily working
# with text data.
library(Matrix)
library(quanteda)
help(package = "quanteda")
# Tokenize SMS text messages.
train.tokens <- tokens(train$Text, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
# Take a look at a specific SMS message and see how it transforms.
train.tokens[[357]]
# Lower case the tokens.
train.tokens <- tokens_tolower(train.tokens)
train.tokens[[357]]
# Use quanteda's built-in stopword list for English.
# NOTE - You should always inspect stopword lists for applicability to
#        your problem/domain.
train.tokens <- tokens_select(train.tokens, stopwords(),
selection = "remove")
train.tokens[[357]]
# Perform stemming on the tokens.
train.tokens <- tokens_wordstem(train.tokens, language = "english")
train.tokens[[357]]
# Create our first bag-of-words model.
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
# Transform to a matrix and inspect.
train.tokens.matrix <- as.matrix(train.tokens.dfm)
View(train.tokens.matrix[1:20, 1:100])
dim(train.tokens.matrix)
# Investigate the effects of stemming.
colnames(train.tokens.matrix)[1:50]
# Per best practices, we will leverage cross validation (CV) as
# the basis of our modeling process. Using CV we can create
# estimates of how well our model will do in Production on new,
# unseen data. CV is powerful, but the downside is that it
# requires more processing and therefore more time.
#
# If you are not familiar with CV, consult the following
# Wikipedia article:
#
#   https://en.wikipedia.org/wiki/Cross-validation_(statistics)
#
# Setup a the feature data frame with labels.
train.tokens.df <- cbind(Label = train$Label, data.frame(train.tokens.dfm))
# Often, tokenization requires some additional pre-processing
names(train.tokens.df)[c(146, 148, 235, 238)]
# Cleanup column names.
names(train.tokens.df) <- make.names(names(train.tokens.df))
# Use caret to create stratified folds for 10-fold cross validation repeated
# 3 times (i.e., create 30 random stratified samples)
set.seed(48743)
cv.folds <- createMultiFolds(train$Label, k = 10, times = 3)
cv.cntrl <- trainControl(method = "repeatedcv", number = 10,
repeats = 3, index = cv.folds)
# Our data frame is non-trivial in size. As such, CV runs will take
# quite a long time to run. To cut down on total execution time, use
# the doSNOW package to allow for multi-core training in parallel.
#
# WARNING - The following code is configured to run on a workstation-
#           or server-class machine (i.e., 12 logical cores). Alter
#           code to suit your HW environment.
#
#install.packages("doSNOW")
library(doSNOW)
# Time the code execution
start.time <- Sys.time()
# Create a cluster to work on 10 logical cores.
cl <- makeCluster(10, type = "SOCK")
registerDoSNOW(cl)
# As our data is non-trivial in size at this point, use a single decision
# tree alogrithm as our first model. We will graduate to using more
# powerful algorithms later when we perform feature extraction to shrink
# the size of our data.
rpart.cv.1 <- train(Label ~ ., data = train.tokens.df, method = "rpart",
trControl = cv.cntrl, tuneLength = 7)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE)
View(slownik)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8")
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8")
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE)
View(slownik)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8"  )
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8"  )
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8"  )
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE )
View(slownik)
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE )
View(slownik)
?read.table
slownik <- read.csv2("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding =  )
slownik <- read.csv2(file="C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding =  )
slownik <- read.csv2("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.file("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
csv
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.delim("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.table("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE, fileEncoding = "UTF-8" )
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE )
View(slownik)
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE,
fileEncoding = "UTF-8")
slownik <- read.csv("C:/Users/siarkmi2/Documents/R/polimorfologik-2.1.txt",
header=F, sep=";", stringsAsFactors=FALSE
)
View(slownik)
stopwordsPL <- readLines("C:/Users/siarkmi2/Documents/GitHub/stopwords/polish.stopwords.txt"
, encoding = "UTF-8")
?tokens_wordstem()
View(slownik)
class(slownik)
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl(10)
pw <- {
"new_user_password"
}
library(forecast)
plot(beerfit1, plot.conf=FALSE, main="Forecasts for quarterly beer production")
dest <- "C:/Users/siarkmi2/OneDrive dla firm 1/NEW_WORLD_ORDER/SGH/MGR/BBreal_01.csv"
sales.raw <- read.csv2(dest)
sales <- tbl_df(sales.raw)
sales[is.na(sales)] <- 0
sales$data <- date(sales$data)
sales$Month <- as.Date(cut(sales$data,
breaks = "month"))
sales$Week <- as.Date(cut(sales$data,
breaks = "week",
start.on.monday = TRUE))
library(ggplot2)
library(scales)
ggplot(data = sales,
aes(Month, sprzedaz)) +
stat_summary(fun.y = mean, # sum adds up all observations for the month
geom = "line") + # "bar" "line"
scale_x_date(
date_labels = "%Y-%m",
date_breaks = "1 month")  # custom x-axis labels
ggplot(data = sales,
aes(Week, sprzedaz)) +
stat_summary(fun.y = mean, # sum adds up all observations for the week
geom = "line") + # "bar" "line"
scale_x_date(
date_labels = "%Y",
date_breaks = "1 week") # custom x-axis labels
sales.xts <-  xts(sales$sprzedaz, as.Date(sales$data, format='%Y-%m-%d'))
monthplot(sales.xts,ylab="$ million",xlab="Month",xaxt="n",
main="Seasonal deviation plot: antidiabetic drug sales")
axis(1,at=1:12,labels=month.abb,cex=0.8)
sales.ts <- ts(sales.raw$sprzedaz,start=c(2015,1,1),frequency=365)
plot.ts(sales.ts)
str(sales.ts)
class(sales.ts)
beer2 <- window(sales.ts)
beerfit1 <- meanf(beer2, h=11)
beerfit2 <- naive(beer2, h=11)
beerfit3 <- snaive(beer2, h=11)
plot(beerfit1, plot.conf=FALSE,
main="Forecasts for quarterly beer production")
lines(beerfit2$mean,col=2)
lines(beerfit3$mean,col=3)
legend("topright",lty=1,col=c(4,2,3),
legend=c("Mean method","Naive method","Seasonal naive method"))
sales$week<-floor_date(sales$data,"week") +8
sales
library(dplyr)
x.ts
x<-ddply(sales, .(week), function(z) mean(z$sprzedaz))
x.ts <- ts(x$V1, start=c(2015,1,1),frequency=52)
plot.ts(x.ts)
beer2 <- window(x.ts)
beerfit1 <- meanf(beer2, h=11)
beerfit2 <- naive(beer2, h=11)
beerfit3 <- snaive(beer2, h=11)
library(forecast)
plot(beerfit1, plot.conf=FALSE, main="Forecasts for quarterly beer production")
lines(beerfit2$mean,col=2)
lines(beerfit3$mean,col=3)
legend("topright",lty=1,col=c(4,2,3),
legend=c("Mean method","Naive method","Seasonal naive method"))
lines(beerfit1$mean,col=1)
plot.ts(x.ts)
lines(beerfit2$mean,col=2)
lines(beerfit3$mean,col=3)
plot(beerfit1, plot.conf=FALSE, main="Forecasts for quarterly beer production")
lines(beerfit2$mean,col=2)
plot(beerfit1, plot.conf=FALSE, main="Forecasts for quarterly beer production")
autoplot(beerfit1, include, PI = TRUE)
autoplot(beerfit1, include, PI = TRUE)
autoplot(beerfit1, PI = TRUE)
plot(beerfit1, PI = TRUE)
plot(beerfit1, PI = FALSE)
plot(beerfit1, PI=FALSE, main="Forecasts for quarterly beer production")
lines(beerfit2$mean,col=2)
lines(beerfit3$mean,col=3)
plot.ts(x.ts, PI=FALSE)
plot.ts(x.ts, PI=FALSE)
x.ts <- ts(x$V1, start=c(2015,1,1),frequency=52)
x<-ddply(sales, .(week), function(z) mean(z$sprzedaz))
library(dplyr)
x<-ddply(sales, .(week), function(z) mean(z$sprzedaz))
library(plyr)
install.packages("plyr")
library(plyr)
library(plyr)
install.packages("plyr")
setwd("C:/Users/siarkmi2/Documents/GitHub/Cursera")
setwd("C:/Users/siarkmi2/Documents/GitHub/Cursera/Getting and Cleaning Data")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
fileName <- "data.zip"
if (!file.exists(fileName)){
download.file(url, fileName)
}
if (!file.exists(fileName) ){
unzip(fileName)
}
unzip(fileName)
??file.path
?file.path
tPh <- "train"
trainSubjects <- read.table(file.path(filePath, tPh , "subject_train.txt"))
filePath <-"UCI HAR Dataset"
trainSubjects <- read.table(file.path(filePath, tPh , "subject_train.txt"))
trainData <- read.table(file.path(filePath, tPh, "Xtrain.txt"))
trainData <- read.table(file.path(filePath, tPh, "X_train.txt"))
trainActivity <- read.table(file.path(filePath, tPh, "y_train.txt"))
tPh <- "test"
trainSubjects <- read.table(file.path(filePath, tPh , "subject_train.txt"))
trainData <-     read.table(file.path(filePath, tPh, "X_train.txt"))
tPh <- "test"
trainSubjects <- read.table(file.path(filePath, tPh , "subject_train.txt"))
trainSubjects <- read.table(file.path(filePath, tPh, "subject_test.txt"))
trainData <-     read.table(file.path(filePath, tPh, "X_test.txt"))
trainActivity <- read.table(file.path(filePath, tPh, "y_test.txt"))
activities <- read.table(file.path(filePath, "activity_labels.txt"))
View(trainActivity)
View(trainActivity)
colnames(activities) <- c("activityId", "activityLabel")
View(activities)
View(activities)
activities <- read.table(file.path(filePath, "activity_labels.txt"))
View(activities)
View(activities)
colnames(activities) <- c("activityId", "activityLabel")
tPh <- "test"
testSubjects <- read.table(file.path(filePath, tPh, "subject_test.txt"))
testData <-     read.table(file.path(filePath, tPh, "X_test.txt"))
testActivity <- read.table(file.path(filePath, tPh, "y_test.txt"))
activities <- read.table(file.path(filePath, "activity_labels.txt"))
colnames(activities) <- c("activityId", "activityLabel")
tPh <- "train"
trainSubjects <- read.table(file.path(filePath, tPh, "subject_train.txt"))
trainData <-     read.table(file.path(filePath, tPh, "X_train.txt"))
trainActivity <- read.table(file.path(filePath, tPh, "y_train.txt"))
humanActivity <- rbind(
cbind(trainSubjects, trainData,trainActivity),
cbind(testSubjects, testData, testActivity)
)
features[,2]
features <- read.table(file.path(filePath, "features.txt"), as.is = TRUE)
View(features)
colnames(humanActivity) <- c("subject", features[,2], "activity")
View(humanActivity)
View(humanActivity)
?grepl
columnsToKeep <- grepl("subject|activity|mean|std", colnames(humanActivity))
humanActivity <- humanActivity[, columnsToKeep]
View(humanActivity)
View(humanActivity)
View(humanActivity)
View(humanActivity)
humanActivity <- rbind(
cbind(trainSubjects, trainData,trainActivity),
cbind(testSubjects, testData, testActivity)
)
colnames(humanActivity) <- c("subject", features[,2], "activity")
columnsToKeep <- grepl("subject|activity|mean()|std", colnames(humanActivity))
humanActivity <- humanActivity[, columnsToKeep]
View(humanActivity)
View(humanActivity)
columnsToKeep <- grepl("subject|activity|mean()|std", colnames(humanActivity))
humanActivity <- humanActivity[, columnsToKeep]
columnsToKeep <- grepl("subject|activity|std", colnames(humanActivity))
columnsToKeep <- grepl("subject|activity|mean()|std", colnames(humanActivity))
columnsToKeep
?greapl
?grepl
?gsub
pattern <- "subject|activity|^mean()$|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
humanActivity <- humanActivity[, columnsToKeep]
columnsToKeep
View(humanActivity)
View(humanActivity)
humanActivity <- rbind(
cbind(trainSubjects, trainData,trainActivity),
cbind(testSubjects, testData, testActivity)
)
colnames(humanActivity) <- c("subject", features[,2], "activity")
pattern <- "subject|activity|mean()|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
humanActivity <- humanActivity[, columnsToKeep]
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
pattern <- "subject|activity|^mean()$|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
View(humanActivity)
View(humanActivity)
pattern <- "subject|activity|mean()|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
pattern <- "subject|activity|\r\nmean\r\n|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
pattern <- "subject|activity|\r\n mean\r\n|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
pattern <- "subject|activity|mean\r\n|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
pattern <- "subject|activity|\\bmean()\\b|std"
columnsToKeep <- grepl(pattern, colnames(humanActivity))
columnsToKeep
humanActivity <- humanActivity[, columnsToKeep]
View(humanActivity)
View(humanActivity)
View(activities)
View(activities)
View(activities)
View(activities)
View(humanActivity)
humanActivity$activity <- factor(humanActivity$activity,
levels = activities[, 1], labels = activities[, 2])
humanActivityCol <- colnames(humanActivity)
humanActivityCol <- gsub("[\\(\\)-]","", humanActivityCol)
humanActivityCol
humanActivityCol <- gsub("[\\(\\)-]","", humanActivityCol)
humanActivityCol <- gsub("^f", "frequencyDomain", humanActivityCols)
humanActivityCol <- gsub("^t", "timeDomain", humanActivityCols)
humanActivityCol <- gsub("Acc", "Accelerometer", humanActivityCols)
humanActivityCol <- gsub("Gyro", "Gyroscope", humanActivityCols)
humanActivityCol <- gsub("Mag", "Magnitude", humanActivityCols)
humanActivityCol <- gsub("Freq", "Frequency", humanActivityCols)
humanActivityCol <- gsub("mean", "Mean", humanActivityCols)
humanActivityCol <- gsub("std", "StandardDeviation", humanActivityCols)
humanActivityCol <- gsub("BodyBody", "Body", humanActivityCols)
humanActivityCol <- gsub("^f", "frequencyDomain", humanActivityCol)
humanActivityCol <- gsub("^t", "timeDomain", humanActivityCol)
humanActivityCol <- gsub("Acc", "Accelerometer", humanActivityCol)
humanActivityCol <- gsub("Gyro", "Gyroscope", humanActivityCol)
humanActivityCol <- colnames(humanActivity)
humanActivityCol <- gsub("[\\(\\)-]","", humanActivityCol)
humanActivityCol <- gsub("^f", "frequencyDomain", humanActivityCol)
humanActivityCol <- gsub("^t", "timeDomain", humanActivityCol)
humanActivityCol <- gsub("Gyro", "Gyroscope", humanActivityCol)
humanActivityCol <- gsub("Mag", "Magnitude", humanActivityCol)
humanActivityCol <- gsub("Freq", "Frequency", humanActivityCol)
humanActivityCol <- gsub("mean", "Mean", humanActivityCol)
humanActivityCol <- gsub("Acc", "Accelerometer", humanActivityCol)
humanActivityCol <- gsub("std", "StandardDeviation", humanActivityCol)
humanActivityCol <- gsub("BodyBody", "Body", humanActivityCol)
humanActivityCol
colnames(humanActivity) <- humanActivityCols
colnames(humanActivity) <- humanActivityCol
View(humanActivity)
View(humanActivity)
humanActivityMeans <- humanActivity %>%
group_by(subject, activity) %>%
summarise_each(funs(mean))
library(dplyr)
humanActivityMeans <- humanActivity %>%
group_by(subject, activity) %>%
summarise_each(funs(mean))
View(humanActivityMeans)
View(humanActivityMeans)
humanActivityMeans <- humanActivity %>%
group_by(subject, activity) %>%
summarise_all(funs(mean))
View(humanActivityMeans)
View(humanActivityMeans)
# group by subject and activity and summarise using mean
humanActivityMeans <- humanActivity %>%
group_by(subject, activity) %>%
summarise_all(funs(mean))
# output to file "tidy_data.txt"
write.table(humanActivityMeans, "tidy_data.txt", row.names = FALSE,
quote = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
View(humanActivityMeans)
View(humanActivityMeans)
humanActivityCol
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
